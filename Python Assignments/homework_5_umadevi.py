# -*- coding: utf-8 -*-
"""Homework_5_Umadevi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YSsA4ah0iZVrcfe2zRvQ86aVauvmmddQ
"""

## Python code trains several machine learning models (Decision Tree, KNN & Neural Networks) to perform the task of classifying online reviews.
## The dataset, taken from "https://dgoldberg.sdsu.edu/515/appliance_reviews.json" contains 1000 online reviews.
## The task is to perform text classification on this dataset to find whether the review refers to safety hazard.


# Importing required libraries
from nltk.util import unique_list
import json, requests, textblob, nltk, time, joblib, google.colab.files, warnings
from math import sqrt
nltk.download('punkt')
import sklearn.metrics, sklearn.model_selection
import sklearn.neighbors, sklearn.neural_network, sklearn.tree
warnings.filterwarnings("ignore")

print()

# Connect to URL
response = requests.get("https://dgoldberg.sdsu.edu/515/appliance_reviews.json")

# Checks if the url is connected successfully
if response:   
    start_time = time.time() # Timestamp for when process started
    print("Loading data...")

    data = json.loads(response.text) # loading data
    
    end_time = time.time() # Timestamp for when process ended
    time_elapsed = end_time - start_time # Difference between times
    print("Completed in", time_elapsed, "seconds.")
    print()

    ## Identifying unique words ##

    start_time = time.time()
    print("Identifying unique words...")

    # Empty list to store values
    reviews_list = []
    reviews = ""
    y = []


    # Extrac reviews ans response variable from json file    
    for line in data:
        review = line["Review"]
        safety_hazard = line["Safety hazard"]

        # Add all reviews to form a text
        reviews = reviews + review.lower() + " "

        # Creat a 2D list of review and safety hazard
        inner_list = [review, safety_hazard]
        reviews_list.append(inner_list)

        # Creating a y-variable
        y.append(safety_hazard)

    # Separate each words in reviews text and find unique words
    blob = textblob.TextBlob(reviews)
    words = blob.words
    unique_words = set(words)
    unique_words = list(unique_words)

    end_time = time.time() 
    time_elapsed = end_time - start_time 
    print("Completed in", time_elapsed, "seconds.")
    print()

    ## Generating relevance scores ##

    start_time = time.time()
    print("Generating relevance scores...")

    # define some variables to compute relevance score   
    relevant_words = []
    A, B, C, D = 0, 0, 0, 0

    # Compute relevance score for each word in unique words list 
    for word in unique_words:
        for line in reviews_list:
            if word in line[0]:
                if line[1] == 1:
                    A = A + 1
                else:
                    B = B + 1
            else:
                if line[1] == 1:
                    C = C + 1
                else:
                    D = D + 1

        try:
            relevance_score = (sqrt(A + B + C + D) * ((A * D) - (C * B))) / sqrt((A + B) * (C + D)) 
        except:
            relevance_score = 0

        # Make a list of relevant words with relenace score atleast of 4000
        if relevance_score >= 4000:
            relevant_words.append(word)
        
        A, B, C, D = 0, 0, 0, 0
    
    end_time = time.time()
    time_elapsed = end_time - start_time
    print("Completed in", time_elapsed, "seconds.")
    print()

    ## Formatting 2D list ##

    start_time = time.time()
    print("Formatting 2D list...")

    # lists to store x values 
    x = []
    in_list = [] 

    for review in reviews_list:
        for word in relevant_words:
            if word in review[0]:
                in_list.append(1)
            else:
                in_list.append(0)
        
        x.append(in_list)
        in_list = []
    
    end_time = time.time() 
    time_elapsed = end_time - start_time 
    print("Completed in", time_elapsed, "seconds.")
    print()

    ## Training machine learning models ##

    start_time = time.time()
    print("Training machine learning models...")

    # Split training versus test data
    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2, random_state = 1)

    # Build decision tree
    dt_clf = sklearn.tree.DecisionTreeClassifier()
    dt_clf = dt_clf.fit(x_train, y_train)

    # Build k-nearest neighbors
    knn_clf = sklearn.neighbors.KNeighborsClassifier(5)
    knn_clf = knn_clf.fit(x_train, y_train)

    # Build neural network
    nn_clf = sklearn.neural_network.MLPClassifier()
    nn_clf = nn_clf.fit(x_train, y_train)

    # Timestamp for when process of training models is done   
    end_time = time.time() 
    time_elapsed = end_time - start_time # Difference between times
    print("Completed in", time_elapsed, "seconds.")
    print()

    # Make predictions and compute accuracy score for decision tree model
    dt_predictions = dt_clf.predict(x_test)
    dt_accuracy = sklearn.metrics.accuracy_score(y_test, dt_predictions)
    print("Decision tree accuracy: ", dt_accuracy)

    # Make predictions and compute accuracy score for k-nearest neighbors model
    knn_predictions = knn_clf.predict(x_test)
    knn_accuracy = sklearn.metrics.accuracy_score(y_test, knn_predictions)
    print("k-nearest neighbors accuracy: ", knn_accuracy)

    # Make predictions and compute accuracy score for neural network model
    nn_predictions = nn_clf.predict(x_test)
    nn_accuracy = sklearn.metrics.accuracy_score(y_test, nn_predictions)
    print("Neural network accuracy: ", nn_accuracy)

    print()

    # Make a lists of models and thier accuracy to find the best model
    accuracy_list = [dt_accuracy, knn_accuracy, nn_accuracy]
    model_names = ["Dicision_Tree", "k_nearest_neighbors", "Neural_Network"]
    models_list = [dt_clf, knn_clf, nn_clf]

    # Find the index number for best accuracy score
    max_accuracy = accuracy_list.index(max(accuracy_list))

    # Export the model with highest accuracy score using joblib
    joblib.dump(models_list[max_accuracy], f"{model_names[max_accuracy]}.joblib")
    google.colab.files.download(f"{model_names[max_accuracy]}.joblib")

    # Print a message that the best model has been saved
    print(f"{model_names[max_accuracy]} model performed best; saved to {model_names[max_accuracy]}.joblib.")

# Print an error message if the program could not connect to URL.
else:
    print("Sorry, connection error.")